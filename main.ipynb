{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\conno\\AppData\\Local\\Temp/ipykernel_8656/1478809248.py:9: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16786826630971706275\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4160159744\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10570738975022763988\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "from spektral.utils import tic, toc\n",
    "from models import *\n",
    "from utils import *\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "print(device_lib.list_local_devices())\n",
    "random.seed(69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(id: str): #DBLP3, DBLP5, Brain, Reddit, DBLPE\n",
    "    dataset_dict=dict()\n",
    "    dataset_dict[\"DBLP3\"]=\"Datasets/DBLP3.npz\"\n",
    "    dataset_dict[\"DBLP5\"]=\"Datasets/DBLP5.npz\"\n",
    "    dataset_dict[\"Brain\"]=\"Datasets/Brain.npz\"\n",
    "    dataset_dict[\"Reddit\"]=\"Datasets/reddit.npz\"\n",
    "    dataset_dict[\"DBLPE\"]=\"Datasets/DBLPE.npz\"\n",
    "\n",
    "    dataset = np.load(dataset_dict[id])\n",
    "    adjs = dataset[\"adjs\"] #(time, node, node)\n",
    "\n",
    "    #Remove nodes with no connections at any timestep\n",
    "    temporal_sum = tf.math.reduce_sum(adjs, axis=0, keepdims=False, name=None)\n",
    "    row_sum = tf.math.reduce_sum(temporal_sum, axis=0, keepdims=False, name=None)\n",
    "    non_zero_indices = np.flatnonzero(row_sum)\n",
    "    adjs = adjs[:,non_zero_indices,:]\n",
    "    adjs = adjs[:,:,non_zero_indices]\n",
    "\n",
    "    #DBLPE is a dynamic featureless graph\n",
    "    if id==\"DBLPE\":\n",
    "        labels = dataset[\"labels\"] #(nodes, time, class)\n",
    "\n",
    "        # labels = np.argmax(labels,axis=2)\n",
    "        labels=labels[non_zero_indices]\n",
    "        feats=np.zeros([adjs.shape[1], adjs.shape[0], adjs.shape[2]])\n",
    "\n",
    "        for i in range(feats.shape[1]):\n",
    "            feats[:,i,:]=np.eye(feats.shape[0])\n",
    "      \n",
    "    #All others are static feature-full graphs\n",
    "    else:\n",
    "        labels = dataset[\"labels\"] #(nodes, class)\n",
    "        feats = dataset[\"attmats\"] #(node, time, feat)\n",
    "\n",
    "        # labels = np.argmax(labels, axis=1)\n",
    "        labels = labels[non_zero_indices]\n",
    "        feats = feats[non_zero_indices]\n",
    "\n",
    "    #Other important variables\n",
    "    n_nodes = adjs.shape[1]\n",
    "    n_timesteps = adjs.shape[0]\n",
    "    n_class = int(labels.shape[1])\n",
    "    n_feat = feats.shape[2]\n",
    "\n",
    "    #Train Val Test split\n",
    "    nodes_id = list(range(n_nodes))\n",
    "    random.shuffle(nodes_id)\n",
    "    idx_train = nodes_id[:(7*n_nodes)//10]\n",
    "    idx_train = [True if i in idx_train else False for i in list(range(n_nodes))]\n",
    "    idx_val = nodes_id[(7*n_nodes)//10: (9*n_nodes)//10]\n",
    "    idx_val = [True if i in idx_val else False for i in list(range(n_nodes))]\n",
    "    idx_test = nodes_id[(9*n_nodes)//10: n_nodes]\n",
    "    idx_test = [True if i in idx_test else False for i in list(range(n_nodes))]\n",
    "\n",
    "    return STG_Dataset(tf.convert_to_tensor(adjs,dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(adjs,dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(feats,dtype=tf.float32), \n",
    "                        tf.convert_to_tensor(feats,dtype=tf.float32), \n",
    "                        tf.convert_to_tensor(labels,dtype=tf.float32), \n",
    "                        tf.convert_to_tensor(labels,dtype=tf.float32), \n",
    "                        n_nodes, n_timesteps, n_class, n_feat, \n",
    "                        np.array(idx_train),\n",
    "                        np.array(idx_val),\n",
    "                        np.array(idx_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train(feats, adjs, labels, idx_train, idx_val, model, loss_fn, optimizer, acc):\n",
    "    #training\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([feats, adjs], training=True)\n",
    "        loss_train = loss_fn(labels[idx_train], predictions[idx_train])\n",
    "    gradients = tape.gradient(loss_train, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    #evaluating\n",
    "    predictions = model([feats, adjs], training=False)\n",
    "    loss_val = loss_fn(labels[idx_val], predictions[idx_val])\n",
    "\n",
    "    acc.update_state(labels[idx_val], predictions[idx_val])\n",
    "\n",
    "    return loss_train\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test(feats, adjs, labels, idx_test, model, loss_fn, optimizer, acc, auc, f1):\n",
    "    predictions = model([feats, adjs], training=False)\n",
    "    loss_test = loss_fn(labels[idx_test], predictions[idx_test])\n",
    "\n",
    "        \n",
    "    acc.update_state(labels[idx_test], predictions[idx_test])\n",
    "    auc.update_state(labels[idx_test], predictions[idx_test])\n",
    "    f1.update_state(labels[idx_test], predictions[idx_test])\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_train_test(epochs, model, data, loss_fn, optimizer, val_acc, acc, auc, f1):\n",
    "    best_val=0\n",
    "    tic()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss_train = train(data.feats_timestep, data.adjs_timestep, data.labels_timestep, data.idx_train, data.idx_val, model, loss_fn, optimizer, val_acc)\n",
    "        if val_acc.result() > best_val:\n",
    "            best_val = val_acc.result()\n",
    "        val_acc.reset_state()\n",
    "    print(f\"Best Training Loss {loss_train}\")\n",
    "    print(f\"Best Val Acc: {best_val}\")\n",
    "\n",
    "    loss_test = test(data.feats_timestep, data.adjs_timestep, data.labels_timestep, data.idx_test, model, loss_fn, optimizer, acc, auc, f1)\n",
    "    print(f\"Test Loss: {loss_test}, Test Acc: {acc.result()}, Test F1 score: {f1.result()}, Auc Test: {auc.result()}\")\n",
    "    print(f\"lambda: {model.trainable_weights[0]}\")\n",
    "    toc(f\"{model.name} ({epochs} epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_train_test(data_id, model_id):\n",
    "    #Constant parameters\n",
    "    epochs = 500\n",
    "    dropout_rate = 0.5\n",
    "    lr = 25e-4\n",
    "    weight_decay = 5e-4\n",
    "    ignores_temporal_data = [\"GAT\", \"GCN\", \"GraphSage\"]\n",
    "\n",
    "    data = read_data(data_id)\n",
    "    model = model_id(data.n_class, data.n_class, dropout_rate)\n",
    "    if (model_id.__name__ in ignores_temporal_data):\n",
    "        model.build([(data.n_nodes, data.n_feat), (data.n_nodes, data.n_nodes)])\n",
    "    else:\n",
    "        model.build([(data.n_nodes, data.n_timestamps, data.n_feat), (data.n_timestamps, data.n_nodes, data.n_nodes)])\n",
    "    model.summary()\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=weight_decay)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    #Metrics can only be created once\n",
    "    val_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    auc = tf.keras.metrics.AUC(num_thresholds=data.adjs.shape[0], multi_label=False)\n",
    "    f1 = tfa.metrics.F1Score(data.labels.shape[1], average=\"weighted\")\n",
    "\n",
    "\n",
    "    for timestep in range(1, data.n_timestamps+1):\n",
    "        data.adjs_timestep = tf.identity(data.adjs[:timestep,:,:])\n",
    "        data.feats_timestep = tf.identity(data.feats)\n",
    "        data.labels_timestep = tf.identity(data.labels)\n",
    "        if (data_id == \"DBLPE\"):\n",
    "            data.labels_timestep = data.labels_timestep[:,timestep-1]\n",
    "        \n",
    "        #If the model ignores temporal data, accumulate adj matrices\n",
    "        if (model_id.__name__ in ignores_temporal_data):\n",
    "            data.adjs_timestep = tf.math.reduce_sum(data.adjs_timestep, axis=0, keepdims=False, name=None)\n",
    "            data.feats_timestep = data.feats_timestep[:, -1, :]\n",
    "\n",
    "            #normalize the adj matrix\n",
    "            data.adjs_timestep += tf.eye(data.adjs_timestep.shape[0])\n",
    "            d = tf.reduce_sum(data.adjs_timestep, axis=1)\n",
    "            normalizing_matrix = np.zeros((data.adjs_timestep.shape[0], data.adjs_timestep.shape[0]))\n",
    "            normalizing_matrix[range(len(normalizing_matrix)), range(len(normalizing_matrix))] = d**(-0.5)\n",
    "            normalizing_matrix = tf.convert_to_tensor(normalizing_matrix, dtype=tf.float32)\n",
    "            data.adjs_timestep = tf.matmul(normalizing_matrix,data.adjs_timestep)\n",
    "            data.adjs_timestep=tf.matmul(tf.matmul(normalizing_matrix,data.adjs_timestep), normalizing_matrix)\n",
    "\n",
    "        timestep_train_test(epochs, model, data, loss_fn, optimizer, val_acc, acc, auc, f1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnngcn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnngnn_layer (RNNGNNLayer)   multiple                  1         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "gcn_conv (GCNConv)           multiple                  505       \n",
      "_________________________________________________________________\n",
      "gcn_conv_1 (GCNConv)         multiple                  30        \n",
      "=================================================================\n",
      "Total params: 536\n",
      "Trainable params: 536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Best Val Acc: 0.6987447738647461\n",
      "Test Loss: 1.0386416912078857, Test Acc: 0.6652719378471375, Test F1 score: 0.5315488576889038, Auc Test: 0.8427197933197021\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.15575035>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 17.56s\n",
      "Best Val Acc: 0.7029288411140442\n",
      "Test Loss: 0.9667765498161316, Test Acc: 0.6610878705978394, Test F1 score: 0.5295363664627075, Auc Test: 0.8560402989387512\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.23945539>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 18.30s\n",
      "Best Val Acc: 0.7008368372917175\n",
      "Test Loss: 0.9611585140228271, Test Acc: 0.6638772487640381, Test F1 score: 0.5390846133232117, Auc Test: 0.8603257536888123\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.15228978>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 20.43s\n",
      "Best Val Acc: 0.6987447738647461\n",
      "Test Loss: 0.978143036365509, Test Acc: 0.6642259359359741, Test F1 score: 0.5448839068412781, Auc Test: 0.8604318499565125\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.12326699>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 21.48s\n",
      "Best Val Acc: 0.6987447738647461\n",
      "Test Loss: 0.9516462087631226, Test Acc: 0.6652719378471375, Test F1 score: 0.5473871231079102, Auc Test: 0.8623735308647156\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.12236278>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 23.94s\n",
      "Best Val Acc: 0.6924686431884766\n",
      "Test Loss: 0.9522597789764404, Test Acc: 0.6638772487640381, Test F1 score: 0.5457972884178162, Auc Test: 0.8637157678604126\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.098534>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 27.22s\n",
      "Best Val Acc: 0.6861924529075623\n",
      "Test Loss: 0.9661000967025757, Test Acc: 0.662881076335907, Test F1 score: 0.5456199049949646, Auc Test: 0.8640356659889221\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.0740693>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 28.60s\n",
      "Best Val Acc: 0.6861924529075623\n",
      "Test Loss: 1.003108263015747, Test Acc: 0.6616109013557434, Test F1 score: 0.5452061891555786, Auc Test: 0.8644784688949585\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.06725935>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 30.17s\n",
      "Best Val Acc: 0.6903765797615051\n",
      "Test Loss: 1.003201961517334, Test Acc: 0.6615527868270874, Test F1 score: 0.5453805923461914, Auc Test: 0.8636465668678284\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.06850874>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 30.82s\n",
      "Best Val Acc: 0.6903765797615051\n",
      "Test Loss: 1.0064435005187988, Test Acc: 0.6606694459915161, Test F1 score: 0.5451893210411072, Auc Test: 0.8634248971939087\n",
      "lambda: <tf.Variable 'lam:0' shape=() dtype=float32, numpy=0.0630974>\n",
      "rnngcn (500 epochs)\n",
      "Elapsed: 32.00s\n"
     ]
    }
   ],
   "source": [
    "overall_train_test(\"DBLP5\", RNNGCN)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "246acc2e86da06095fd9b40717c2339d803f4778ff32430783cde5272a877e12"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('deep_learning': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
