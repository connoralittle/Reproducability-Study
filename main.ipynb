{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas\n",
    "import spektral\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from spektral.utils import tic, toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.6.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\conno\\appdata\\roaming\\python\\python38\\site-packages\n",
      "Requires: wheel, numpy, tensorflow-estimator, h5py, six, absl-py, astunparse, clang, wrapt, google-pasta, flatbuffers, termcolor, opt-einsum, keras, tensorboard, gast, grpcio, typing-extensions, protobuf, keras-preprocessing\n",
      "Required-by: spektral\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2679603500104633918\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4160159744\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13292143627832581356\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatio Temporal Graph Dataset\n",
    "@dataclass()\n",
    "class STG_Dataset:\n",
    "    adjs: np.ndarray\n",
    "    adjs_timestep: np.ndarray\n",
    "    feats: np.ndarray\n",
    "    feats_timestep: np.ndarray\n",
    "    labels: np.ndarray\n",
    "    labels_timestep: np.ndarray\n",
    "\n",
    "    n_nodes: int\n",
    "    n_timestamps: int\n",
    "    n_class: int\n",
    "    n_feat: int\n",
    "\n",
    "    idx_train: np.ndarray\n",
    "    idx_val: np.ndarray\n",
    "    idx_test: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(id: str): #DBLP3, DBLP5, Brain, Reddit, DBLPE\n",
    "    dataset_dict=dict()\n",
    "    dataset_dict[\"DBLP3\"]=\"Datasets/DBLP3.npz\"\n",
    "    dataset_dict[\"DBLP5\"]=\"Datasets/DBLP5.npz\"\n",
    "    dataset_dict[\"Brain\"]=\"Datasets/Brain.npz\"\n",
    "    dataset_dict[\"Reddit\"]=\"Datasets/reddit.npz\"\n",
    "    dataset_dict[\"DBLPE\"]=\"Datasets/DBLPE.npz\"\n",
    "\n",
    "    dataset = np.load(dataset_dict[id])\n",
    "    adjs = dataset[\"adjs\"] #(time, node, node)\n",
    "\n",
    "    #Remove nodes with no connections at any timestep\n",
    "    temporal_sum = tf.math.reduce_sum(adjs, axis=0, keepdims=False, name=None)\n",
    "    row_sum = tf.math.reduce_sum(temporal_sum, axis=0, keepdims=False, name=None)\n",
    "    non_zero_indices = np.flatnonzero(row_sum)\n",
    "    adjs = adjs[:,non_zero_indices,:]\n",
    "    adjs = adjs[:,:,non_zero_indices]\n",
    "\n",
    "    #DBLPE is a dynamic featureless graph\n",
    "    if id==\"DBLPE\":\n",
    "        labels = dataset[\"labels\"] #(nodes, time, class)\n",
    "\n",
    "        # labels = np.argmax(labels,axis=2)\n",
    "        labels=labels[non_zero_indices]\n",
    "        feats=np.zeros([adjs.shape[1], adjs.shape[0], adjs.shape[2]])\n",
    "\n",
    "        for i in range(feats.shape[1]):\n",
    "            feats[:,i,:]=np.eye(feats.shape[0])\n",
    "      \n",
    "    #All others are static feature-full graphs\n",
    "    else:\n",
    "        labels = dataset[\"labels\"] #(nodes, class)\n",
    "        feats = dataset[\"attmats\"] #(node, time, feat)\n",
    "\n",
    "        # labels = np.argmax(labels, axis=1)\n",
    "        labels = labels[non_zero_indices]\n",
    "        feats = feats[non_zero_indices]\n",
    "\n",
    "    #Other important variables\n",
    "    n_nodes = adjs.shape[1]\n",
    "    n_timesteps = adjs.shape[0]\n",
    "    n_class = int(labels.shape[1])\n",
    "    n_feat = feats.shape[2]\n",
    "\n",
    "    #Train Val Test split\n",
    "    nodes_id = list(range(n_nodes))\n",
    "    random.shuffle(nodes_id)\n",
    "    idx_train = nodes_id[:(7*n_nodes)//10]\n",
    "    idx_train = [True if i in idx_train else False for i in list(range(n_nodes))]\n",
    "    idx_val = nodes_id[(7*n_nodes)//10: (9*n_nodes)//10]\n",
    "    idx_val = [True if i in idx_val else False for i in list(range(n_nodes))]\n",
    "    idx_test = nodes_id[(9*n_nodes)//10: n_nodes]\n",
    "    idx_test = [True if i in idx_test else False for i in list(range(n_nodes))]\n",
    "\n",
    "    return STG_Dataset(tf.convert_to_tensor(adjs,dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(adjs,dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(feats,dtype=tf.float32), \n",
    "                        tf.convert_to_tensor(feats,dtype=tf.float32), \n",
    "                        tf.convert_to_tensor(labels,dtype=tf.float32), \n",
    "                        tf.convert_to_tensor(labels,dtype=tf.float32), \n",
    "                        n_nodes, n_timesteps, n_class, n_feat, \n",
    "                        np.array(idx_train),\n",
    "                        np.array(idx_val),\n",
    "                        np.array(idx_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gat(tf.keras.Model):\n",
    "    def __init__(self, nhid, nclass, dropout):\n",
    "        super(Gat, self).__init__()\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.gat1 = spektral.layers.GATConv(channels=nhid,attn_heads=1,concat_heads=True, activation='relu')\n",
    "        self.gat2 = spektral.layers.GATConv(channels=nclass,attn_heads=1,concat_heads=True, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        feats, adj = inputs\n",
    "        x_1 = self.gat1([feats, adj])\n",
    "        dropout = self.dropout(x_1, training=training)\n",
    "        return self.gat2([dropout, adj])\n",
    "\n",
    "    # def model(self, n_feat, n_nodes):\n",
    "    #     x_in = tf.keras.layers.Input(shape=(n_feat,))\n",
    "    #     a_in = tf.keras.layers.Input(shape=(None,), sparse=True)\n",
    "    #     return tf.keras.Model(inputs=[x_in, a_in], outputs=self.call([x_in, a_in]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train(feats, adjs, labels, idx_train, idx_val, model, loss_fn, optimizer, acc):\n",
    "    #training\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([feats, adjs], training=True)\n",
    "        loss_train = loss_fn(labels[idx_train], predictions[idx_train])\n",
    "    gradients = tape.gradient(loss_train, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    #evaluating\n",
    "    predictions = model([feats, adjs], training=False)\n",
    "    loss_val = loss_fn(labels[idx_val], predictions[idx_val])\n",
    "\n",
    "    acc.update_state(labels[idx_val], predictions[idx_val])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test(feats, adjs, labels, idx_test, model, loss_fn, optimizer, acc, auc, f1):\n",
    "    predictions = model([feats, adjs], training=False)\n",
    "    loss_test = loss_fn(labels[idx_test], predictions[idx_test])\n",
    "\n",
    "        \n",
    "    acc.update_state(labels[idx_test], predictions[idx_test])\n",
    "    auc.update_state(labels[idx_test], predictions[idx_test])\n",
    "    f1.update_state(labels[idx_test], predictions[idx_test])\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_train_test(epochs, model, data, loss_fn, optimizer, val_acc, acc, auc, f1):\n",
    "    best_val=0\n",
    "    tic()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(data.feats_timestep, data.adjs_timestep, data.labels_timestep, data.idx_train, data.idx_val, model, loss_fn, optimizer, val_acc)\n",
    "        if val_acc.result() > best_val:\n",
    "            best_val = val_acc.result()\n",
    "        val_acc.reset_state()\n",
    "    print(f\"Best Val Acc: {best_val}\")\n",
    "\n",
    "    loss_test = test(data.feats_timestep, data.adjs_timestep, data.labels_timestep, data.idx_test, model, loss_fn, optimizer, acc, auc, f1)\n",
    "    print(f\"Test Loss: {loss_test}, Test Acc: {acc.result()}, Test F1 score: {f1.result()}, Auc Test: {auc.result()}\")\n",
    "    toc(f\"{model.name} ({epochs} epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_train_test(data_id, model_id):\n",
    "    #Constant parameters\n",
    "    epochs = 500\n",
    "\n",
    "    data = read_data(data_id)\n",
    "    model = model_id(data.n_class, data.n_class, 0.5)\n",
    "    model.build([(data.n_nodes, data.n_feat), (data.n_nodes, data.n_nodes)])\n",
    "    model.summary()\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=25e-4, weight_decay=5e-4)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    val_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    auc = tf.keras.metrics.AUC(num_thresholds=data.adjs.shape[0], multi_label=True)\n",
    "    f1 = tfa.metrics.FBetaScore(data.labels.shape[1], average=\"weighted\")\n",
    "\n",
    "    for timestep in range(data.n_timestamps + 1):\n",
    "        data.adjs_timestep = tf.identity(data.adjs[:timestep,:,:])\n",
    "        data.feats_timestep = tf.identity(data.feats)\n",
    "        data.labels_timestep = tf.identity(data.labels)\n",
    "        if (data_id == \"DBLPE\"):\n",
    "            data.labels_timestep = data.labels_timestep[:,timestep - 1]\n",
    "        \n",
    "        #If the model ignores temporal data, accumulate adj matrices\n",
    "        if (model_id.__name__ in [\"Gat\"]):\n",
    "            data.adjs_timestep = tf.math.reduce_sum(data.adjs_timestep, axis=0, keepdims=False, name=None)\n",
    "            data.feats_timestep = data.feats_timestep[:, -1, :]\n",
    "\n",
    "            #normalize the adj matrix\n",
    "            data.adjs_timestep += tf.eye(data.adjs_timestep.shape[0])\n",
    "            d = tf.reduce_sum(data.adjs_timestep, axis=1)\n",
    "            normalizing_matrix = np.zeros((data.adjs_timestep.shape[0], data.adjs_timestep.shape[0]))\n",
    "            normalizing_matrix[range(len(normalizing_matrix)), range(len(normalizing_matrix))] = d**(-0.5)\n",
    "            normalizing_matrix = tf.convert_to_tensor(normalizing_matrix, dtype=tf.float32)\n",
    "            data.adjs_timestep = tf.matmul(normalizing_matrix,data.adjs_timestep)\n",
    "            data.adjs_timestep=tf.matmul(tf.matmul(normalizing_matrix,data.adjs_timestep), normalizing_matrix)\n",
    "\n",
    "        timestep_train_test(epochs, model, data, loss_fn, optimizer, val_acc, acc, auc, f1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gat\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "gat_conv (GATConv)           multiple                  309       \n",
      "_________________________________________________________________\n",
      "gat_conv_1 (GATConv)         multiple                  18        \n",
      "=================================================================\n",
      "Total params: 327\n",
      "Trainable params: 327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Best Val Acc: 0.8042704463005066\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4868/2605087678.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moverall_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DBLP3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4868/2159931125.py\u001b[0m in \u001b[0;36moverall_train_test\u001b[1;34m(data_id, model_id)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjs_timestep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalizing_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjs_timestep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizing_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtimestep_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4868/4125278566.py\u001b[0m in \u001b[0;36mtimestep_train_test\u001b[1;34m(epochs, model, data, loss_fn, optimizer, val_acc, acc, auc, f1)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeats_timestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjs_timestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_timestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Test Loss: {loss_test}, Test Acc: {acc.result()}, Test F1 score: {f1.result()}, Auc Test: {auc.result()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{model.name} ({epochs} epochs)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_test' is not defined"
     ]
    }
   ],
   "source": [
    "overall_train_test(\"DBLP3\", Gat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "246acc2e86da06095fd9b40717c2339d803f4778ff32430783cde5272a877e12"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('deep_learning': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
